{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc255dd2-0ee4-4308-b2cc-173178ea709c",
   "metadata": {},
   "source": [
    "# Lecture 28 – CSCI 3022\n",
    "\n",
    "\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/fa23/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81168b67-b5ff-4005-8c06-608e07f2c7b2",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of applying **feature functions** to generate new features for use in modeling. In this notebook, we will discuss:\n",
    "* Feature transformations\n",
    "* One-hot encoding\n",
    "* Polynomial features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e8198",
   "metadata": {},
   "source": [
    "# Ex 1:  Bay Area Housing Data\n",
    "\n",
    "For our first example we consider housing data scraped from the San Francisco Chronicle (SFChron) website. We restrict the data to houses sold in 2006, when sale prices were relatively stable, so we don’t need to account for trends in price. Since we have no plans to generalize our findings beyond the time period and the location and we are working with a census, the population matches the access frame and the sample consists of the entire population.\n",
    "\n",
    "As for granularity, each record represents a sale of a home in the SF Bay Area during the specified time period. This means that if a home was sold twice during this time, then there are two records in the table. And if a home in the Bay Area was not up for sale during this time, then it does not appear in the dataset.\n",
    "\n",
    "The data are in `BayAreaHousing.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629de4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh=pd.read_csv(\"BayAreaHousing.csv\")\n",
    "\n",
    "sfh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0e067",
   "metadata": {},
   "source": [
    "The data consists of the following columns:\n",
    "\n",
    "`date`:  Date the house sold\n",
    "\n",
    "`price`:  Price the house sold for\n",
    "\n",
    "`city`:  In this case we consider 4 cities: Richmond, Berkeley, Piedmont and a combination of 3 cities Lafayette, Moraga and Morinda that we name Lamorinda\n",
    "\n",
    "`br`:  Number of bedrooms\n",
    "\n",
    "`lsqft`:  Square feet of the lot\n",
    "\n",
    "`bsqft`:  Square feet of the house\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b4bc7",
   "metadata": {},
   "source": [
    "## Choosing Features to Predict Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution of Sale Price - look for outliers or other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sfh[\"price\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The skewed distribution is hinting that our models will likely do better\n",
    "# if we transform price to log(price)  (so that outliers don't overly influence the model)\n",
    "plt.hist(np.log(sfh[\"price\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb2494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39ab8391",
   "metadata": {},
   "source": [
    "### Correlation: \n",
    "\n",
    "We can start by calculating the correlation between price and the available quantitative explantory variables, to see which one(s) it correlates most highly with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88315571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699678ef",
   "metadata": {},
   "source": [
    "Sale price correlates most highly with house size, called `bsqft` for building square feet. \n",
    "We confirm from our scatter plot of sale price against house size that the association is linear. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3752c0",
   "metadata": {},
   "source": [
    "### Visualizing Relationships between Price and possible explanatory quantitative features:\n",
    "\n",
    "#### Price vs Square feet of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c314e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the function below to plot all combinations at once:\n",
    "#sns.pairplot(sfh)\n",
    "\n",
    "px.scatter(sfh,sfh[\"bsqft\"],sfh[\"price\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406ad09",
   "metadata": {},
   "source": [
    "#### Price vs Square feet of the lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025113a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "px.scatter(sfh,sfh[\"lsqft\"],sfh[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fe8a9",
   "metadata": {},
   "source": [
    "#### Price vs Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563174ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(sfh,sfh[\"br\"],sfh[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524e3e6",
   "metadata": {},
   "source": [
    "### Caution When not to use scatterplot:\n",
    "Due to overplotting, it's better to use boxplots (or violin plots) to examine associations between variables when one of your variables is categorical or discrete with only a few values.\n",
    "**CAVEAT** A boxplot/violin plot can be misleading if you only have a few data points for a given category - so notice for 7 or more bedrooms we should just plots the datapoints, and not the boxplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(sfh,x=sfh[\"br\"],y=sfh[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd6a4e",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "### Examining feature that is most correlated with price:  bsqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e333557",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(sfh,sfh[\"bsqft\"],sfh[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14960fc",
   "metadata": {},
   "source": [
    "The relationship between price and bsqft does look roughly linear, but the very large and expensive houses are far from the center of the distribution and can overly influence the model. \n",
    "Let's try a **log transformation of both variables** to make the distributions of price and size more symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d48248",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh[\"log_price\"]=np.log(sfh[\"price\"])\n",
    "sfh[\"log_bsqft\"]=np.log(sfh[\"bsqft\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d858e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(sfh,sfh[\"log_bsqft\"],sfh[\"log_price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4e433",
   "metadata": {},
   "source": [
    "## First Version of Model:  Using the (transformed) predictor with the highest correlation to log(price)\n",
    "\n",
    "Let's create a model of the form:  $$\\text{log(price)} = \\theta_0 + \\theta_1 \\text{log(bsqft)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb7efb",
   "metadata": {},
   "source": [
    "Ideally, a model that uses transformations should make sense in the context of the data. If we fit a simple linear model based on log(size), then when we examine the coefficient, we think in terms of a percentage increase. For example, a doubling of $x$ increases the prediction by $\\theta \\log(2)$, since $\\theta \\log(2x) = \\theta\\log(2) + \\theta \\log(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b32ab7",
   "metadata": {},
   "source": [
    "Let's begin by fitting a model that explains log-transformed price by the house's log-transformed size. But first, we note that this model is still considered a linear model. If we represent sale price by $y$ and house size by $x$, then the model is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(y) ~&=~ \\theta_0 + \\theta_1\\log(x) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "(Note that we have ignored the approximation in this equation to make the linear relationship clearer.) This equation may not seem linear, but if we rename $\\log(y)$ to $w$ and $\\log(x)$ to $v$, then we can express this \"log–log\" relationship as a linear model in $w$ and $v$:\n",
    "\n",
    "$$\n",
    "w ~=~ \\theta_0 + \\theta_1 v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77715baf",
   "metadata": {},
   "source": [
    "Other examples of models that can be expressed as linear combinations of transformed features are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(y) ~&=~ \\theta_0 + \\theta_1 x \\\\\n",
    "y ~&=~ \\theta_0 + \\theta_1 x + \\theta_2 x^2 \\\\\n",
    "y ~&=~ \\theta_0 + \\theta_1 x + \\theta_2 z  + \\theta_3 x z  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Again, if we rename $\\log(y)$ to $w$, $x^2$ to $u$, and $x z$ as $t$, then we can express each of these models as linear in these renamed features. In order, the preceding models are now: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w ~&=~ \\theta_0 + \\theta_1 x \\\\\n",
    "y ~&=~ \\theta_0 + \\theta_1 x + \\theta_2 u\\\\\n",
    "y ~&=~ \\theta_0 + \\theta_1 x + \\theta_2 z  + \\theta_3 t \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In short, we can think of models that include nonlinear transformations of features and/or combinations of features as linear in their derived features. In practice, we don't rename the transformed features when we describe the model; instead, we write the model using the transformations of the original features because it's important to keep track of them, especially when interpreting the coefficients and checking residual plots.\n",
    "\n",
    "When we refer to these models, we include mention of the transformations. That is, we call a model *log–log* when both the outcome and explanatory variables are log-transformed; we say it's *log–linear* when the outcome is log-transformed but not the explanatory variable; we describe a model as having *polynomial features* of, say, degree two, when the first and second power transformations of the explanatory variable are included; and we say a model includes an *interaction term* between two explanatory features when the product of these two features is included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a8315",
   "metadata": {},
   "source": [
    "Let's fit a log–log model of price on size:\n",
    "\n",
    "\n",
    "i.e. we'll create a model of the form:  $$\\text{log(price)} = \\theta_0 + \\theta_1 \\text{log(bsqft)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8eb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "X1_log = sfh[['log_bsqft']]    \n",
    "y_log = sfh['log_price']\n",
    "\n",
    "model1 =lm.LinearRegression().fit(X1_log, y_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ae0c7",
   "metadata": {},
   "source": [
    "**Output Model Coefficients**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081db587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output the coefficients of the model:\n",
    "pd.DataFrame({\"Feature\":[X1_log.columns[0],\"intercept\"], \"Model Coefficient\":[model1.coef_[0],model1.intercept_]}).set_index(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005e60c",
   "metadata": {},
   "source": [
    "**Output RMSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4dc3c8",
   "metadata": {},
   "source": [
    "The coefficients and predicted values from this model cannot be directly compared to a model fitted using linear features because the units are the log of dollars and log of square feet, not dollars and square feet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE\n",
    "y=sfh['price']\n",
    "\n",
    "y_hat_m1 = np.exp(model1.predict(X1_log))\n",
    "\n",
    "print(f\"The RMSE of the model is {np.sqrt(np.mean((y-y_hat_m1)**2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f23e0",
   "metadata": {},
   "source": [
    "**Plot Residuals**\n",
    "\n",
    "Next, we examine the residuals and predicted values with a plot.  Note, when examining residuals we will keep the log/log scale as it's easier to see patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d72793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep this in a log scale to better notice any patterns\n",
    "\n",
    "prediction = model1.predict(X1_log)\n",
    "error = sfh['log_price'] - prediction \n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "ax[0].scatter(sfh[\"log_bsqft\"],sfh[\"log_price\"])\n",
    "ax[0].plot(sfh[\"log_bsqft\"], prediction, 'r--', label =r'$log(price) = {0:.2f}log(bsqft)+{1:.2f}$'.format(model1.coef_[0],model1.intercept_))\n",
    "ax[0].set_xlabel('log_bsqft')\n",
    "ax[0].set_ylabel('log_price')\n",
    "ax[0].set_title('SLR')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "\n",
    "ax[1].scatter(prediction, error)\n",
    "ax[1].axhline(0, c='black', linewidth=1)\n",
    "ax[1].set_xlabel(r'$log(\\hat{y})$')\n",
    "ax[1].set_ylabel(r'Residuals: $log(y) - log(\\hat{y})$');\n",
    "ax[1].set_title(\"Residuals vs. log(predicted price) (both in log units)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f815e",
   "metadata": {},
   "source": [
    "The residual plot looks reasonable, but it contains thousands of points, which makes it hard to see curvature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e0fa5",
   "metadata": {},
   "source": [
    "Another way to examine the residuals is to plot the residuals vs actual price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=sfh['log_price'], y=error,\n",
    "                 labels=dict(x='log(Actual sale price)', y='log(actual) - log(predicted)'))\n",
    "\n",
    "fig.add_hline(0, line_width=2, line_dash='dash', opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102787e",
   "metadata": {},
   "source": [
    "Notice that this shows that the model is _________ lower priced homes (predicting a Sale Price ________ than the actual Sale Price) and _______  higher priced homes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31598b25",
   "metadata": {},
   "source": [
    "### Coefficient of Determination (i.e. Multiple R^2)\n",
    "\n",
    "In SLR, $r^{2}$ (correlation squared) and Multiple $R^{2}$ are\n",
    "equivalent.  \n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$\n",
    "\n",
    "$R^2$  can be used\n",
    "in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature.  \n",
    "\n",
    "This number summarizes the amount of variation in the price that is being captured by the model.  The closer this is to 1, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d747ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_m1 = np.var(model1.predict(X1_log)) / np.var(sfh[\"log_price\"])\n",
    "\n",
    "print('Multiple R^2 for model1:  ', r2_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in function to calculate this:\n",
    "model1.score(X1_log, sfh[\"log_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh.corr().loc[\"log_price\",\"log_bsqft\"]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cd5a8",
   "metadata": {},
   "source": [
    "## Feature Engineering:  Adding Qualitative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b2013",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What about qualitative features?  There isn't a way to calculate a \"correlation\" between price and city. \n",
    "\n",
    "To see if additional qualitative variables might be helpful, we can plot the residuals from the fitted model against a variable that is not in the model. If we see patterns, that indicates we might want to include this additional feature or a transformation of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdba7d0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "prediction = model1.predict(X1_log)\n",
    "error = sfh['log_price'] - prediction \n",
    "\n",
    "\n",
    "sfh = sfh.assign(errors1_log=error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8af9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(sfh, x='city', y='errors1_log',\n",
    "       category_orders={\"city\":[\"Piedmont\",\"Lamorinda\",\"Berkeley\", \"Richmond\"]},\n",
    "       labels=dict(errors1_log='Residuals (log USD)', city=''))\n",
    "\n",
    "fig.add_hline(0, line_width=2, line_dash='dash', opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd69597",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig=px.box(sfh, x='city', y='errors1_log',\n",
    "       category_orders={\"city\":[\"Piedmont\",\"Lamorinda\",\"Berkeley\", \"Richmond\"]},\n",
    "       labels=dict(errors1_log='Residuals (log USD)', city=''))\n",
    "\n",
    "fig.add_hline(0, line_width=2, line_dash='dash', opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f193e69",
   "metadata": {},
   "source": [
    "This plot shows us that the distribution of errors appears shifted by city. Ideally, the median of each city's box plot lines up with 0 on the y-axis (meaning there was no difference in prediction by city). Instead, more than 75%  of the houses sold in  Piedmont have positive errors, meaning the actual sale price is above the predicted value. And at the other extreme, more than 75% of sale prices in Richmond fall below their predicted values. These patterns suggest that we should include city in the model. From a context point of view, it makes sense for location to impact sale price. In the next section, we show how to incorporate a nominal variable into a linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fefbb1-3125-42f5-ab5d-4dfd0c36e961",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a feature engineering technique to generate numeric feature from categorical data. For example, we can use one-hot encoding to incorporate the day of the week as an input into a regression model.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we want to use a design matrix of 2 features – the `log_bsqft` and `city` – to predict the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = sfh[[\"log_bsqft\",\"city\"]]\n",
    "y = sfh[\"log_price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d683b1-a7cf-45ed-bf65-1b4ebb66da50",
   "metadata": {},
   "source": [
    "Because `city` is non-numeric, we will apply one-hot encoding before fitting a model.\n",
    "\n",
    "The `OneHotEncoder` class of `sklearn` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out)) offers a quick way to perform one-hot encoding. For now, recognize that we follow a very similar workflow to when we were working with the `LinearRegression` class: we initialize a `OneHotEncoder` object, fit it to our data, then use `.transform` to apply the fitted encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc958780-bfb8-45c7-a33a-27a9b6d276d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize a OneHotEncoder object\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder\n",
    "ohe.fit(sfh[[\"city\"]])\n",
    "\n",
    "# Use the encoder to transform the raw \"day\" feature and put in a new dataframe\n",
    "\n",
    "encoded_city_df = pd.DataFrame(ohe.transform(sfh[['city']]).todense(), \n",
    "                           columns=ohe.get_feature_names_out(),\n",
    "                           index = sfh.index)\n",
    "\n",
    "\n",
    "encoded_city_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68105ccb-6f42-4b38-9b76-6752b7c7df72",
   "metadata": {},
   "source": [
    "The `OneHotEncoder` has converted the categorical `city` feature into four numeric features! \n",
    "\n",
    "Let's join this one-hot encoding to the original data to form our featurized design matrix. We drop the original `city` column so our design matrix only includes numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea867e6b-742a-4627-95a2-f0f80d1a0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_raw.join(encoded_city_df).drop(columns=\"city\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6dc523-b40c-45f7-8bf4-c74bd4e6f375",
   "metadata": {},
   "source": [
    "Now, we can use `sklearn`'s `LinearRegression` class to fit a model to this design matrix.\n",
    "\n",
    "\n",
    "We're fitting a model of this form:\n",
    "\n",
    "$$log(price) = \\theta_1log(bsqft)+\\theta_2(Berkeley)+\\theta_3(Lamorinda)+\\theta_4(Piedmont)+\\theta_5(Richmond)$$\n",
    "\n",
    "\n",
    "\n",
    "Notice, this is equivalent to fitting 4 models with the same slope, but the intercept term depending on city:\n",
    "\n",
    "$$log(price) = \\theta_1log(bsqft)+\\theta_2 \\hspace{5mm} \\text{     (for houses in Berkeley)}$$\n",
    "\n",
    "$$log(price) = \\theta_1log(bsqft)+\\theta_3  \\hspace{5mm} \\text{     (for houses in Lamorinda)}$$\n",
    "\n",
    "$$log(price) = \\theta_1log(bsqft)+\\theta_4  \\hspace{5mm} \\text{     (for houses in Piedmont)}$$\n",
    "\n",
    "$$log(price) = \\theta_1log(bsqft)+\\theta_5 \\hspace{5mm}  \\text{     (for houses in Richmond)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bcc7e-642b-4cf7-bb7b-5d74adc24f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sfh[\"log_price\"]\n",
    "\n",
    "ohe_model = lm.LinearRegression(fit_intercept=False) \n",
    "\n",
    "#Since we are using one-hot encoding, tell sklearn to not add an additional intercept column. \n",
    "\n",
    "ohe_model.fit(X, y)\n",
    "\n",
    "pd.DataFrame({\"Feature\":X.columns, \"Model Coefficient\":ohe_model.coef_}).set_index(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1557fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE\n",
    "y=sfh['price']\n",
    "\n",
    "y_hat_m2 = np.exp(ohe_model.predict(X))\n",
    "\n",
    "print(f\"The RMSE of the model is {np.sqrt(np.mean((y-y_hat_m2)**2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8637a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll keep this in a log scale to better notice any patterns\n",
    "prediction = ohe_model.predict(X)\n",
    "error = sfh['log_price'] - prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abe7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=prediction, y=error,\n",
    "                 labels=dict(x='Predicted sale price (log USD)', y='Error'))\n",
    "\n",
    "fig.add_hline(0, line_width=2, line_dash='dash', opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ede9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R-square for city and log(size):\",\n",
    "      f\" {ohe_model.score(X, sfh['log_price']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ac1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=sfh['log_price'], y=error,\n",
    "                 labels=dict(x='Actual sale price (log USD)', y='Error (Log USD)'))\n",
    "\n",
    "fig.add_hline(0, line_width=2, line_dash='dash', opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a493da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dd63060",
   "metadata": {},
   "source": [
    "# Ex 2:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0615660-3122-4181-aeb9-ef886cea19ea",
   "metadata": {},
   "source": [
    "### Feature Engineering:  Adding Polynomial Features\n",
    "\n",
    "Consider the `vehicles` dataset, which includes information about cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716c468-74e5-4960-864b-40f2e8af5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = sns.load_dataset(\"mpg\").dropna().rename(columns = {\"horsepower\": \"hp\"}).sort_values(\"hp\")\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27378d6-516e-417a-b80a-0282ea4243df",
   "metadata": {},
   "source": [
    "Suppose we want to use the `hp` (horsepower) of a car to predict its `mpg` (gas mileage in miles per gallon). If we visualize the relationship between these two variables, we see a non-linear curvature. Fitting a linear model to these variables results in a high (poor) value of RMSE. \n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33629843-f23d-4624-accf-a9b6af482d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "y = vehicles[\"mpg\"]\n",
    "\n",
    "hp_model = lm.LinearRegression()\n",
    "hp_model.fit(X, y)\n",
    "hp_model_predictions = hp_model.predict(X)\n",
    "\n",
    "sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "\n",
    "plt.plot(vehicles[\"hp\"], hp_model_predictions, c=\"tab:red\");\n",
    "\n",
    "print(f\"MSE of model with (hp) feature: {np.mean((y-hp_model_predictions)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249691a7-ea35-493c-ad04-f224e61f50c4",
   "metadata": {},
   "source": [
    "To capture the non-linear relationship between the variables, we can introduce a non-linear feature: `hp` squared. Our new model is:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp}^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f79d3-df91-408e-b78d-3ba61a7a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "X.loc[:, \"hp^2\"] = vehicles[\"hp\"]**2\n",
    "\n",
    "hp2_model = lm.LinearRegression()\n",
    "hp2_model.fit(X, y)\n",
    "hp2_model_predictions = hp2_model.predict(X)\n",
    "\n",
    "sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "\n",
    "plt.plot(vehicles[\"hp\"], hp2_model_predictions, c=\"tab:red\");\n",
    "\n",
    "print(f\"MSE of model with (hp^2) feature: {np.mean((y-hp2_model_predictions)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337bad5-873a-48ca-aa70-ba2467b60619",
   "metadata": {},
   "source": [
    "What if we take things further and add even *more* polynomial features?\n",
    "\n",
    "The cell below fits models of increasing complexity and computes their MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032da0c-757a-4472-940c-e72718bc514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, observations):\n",
    "    return np.mean((observations - predictions)**2)\n",
    "\n",
    "# Add hp^3 and hp^4 as features to the data\n",
    "X[\"hp^3\"] = vehicles[\"hp\"]**3\n",
    "X[\"hp^4\"] = vehicles[\"hp\"]**4\n",
    "\n",
    "# Fit a model with order 3\n",
    "hp3_model = lm.LinearRegression()\n",
    "hp3_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\"]], vehicles[\"mpg\"])\n",
    "hp3_model_predictions = hp3_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\"]])\n",
    "\n",
    "# Fit a model with order 4\n",
    "hp4_model = lm.LinearRegression()\n",
    "hp4_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]], vehicles[\"mpg\"])\n",
    "hp4_model_predictions = hp4_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]])\n",
    "\n",
    "# Plot the models' predictions\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "predictions_dict = {0:hp2_model_predictions, 1:hp3_model_predictions, 2:hp4_model_predictions}\n",
    "\n",
    "for i in predictions_dict:\n",
    "    ax[i].scatter(vehicles[\"hp\"], vehicles[\"mpg\"], edgecolor=\"white\", lw=0.5)\n",
    "    ax[i].plot(vehicles[\"hp\"], predictions_dict[i], \"tab:red\")\n",
    "    ax[i].set_title(f\"Model with order {i+2}\")\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].annotate(f\"MSE: {np.round(mse(vehicles['mpg'], predictions_dict[i]), 3)}\", (120, 40))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266b33f-37c6-4d39-b19c-f18fc576b19a",
   "metadata": {},
   "source": [
    "## Complexity and Overfitting\n",
    "\n",
    "What we saw above was the phenomenon of **model complexity** – as we add additional features to the design matrix, the model becomes increasingly *complex*. Models with higher complexity have lower values of training error. Intuitively, this makes sense: with more features at its disposal, the model can match the observations in the trainining data more and more closely. \n",
    "\n",
    "We can run an experiment to see this in action. In the cell below, we fit many models of progressively higher complexity, then plot the MSE of predictions on the training set. The code used (specifically, the `Pipeline` and `PolynomialFeatures` functions of `sklearn`) is out of scope.\n",
    "\n",
    "The **order** of a polynomial model is the highest power of any term in the model. An order 0 model takes the form $\\hat{y} = \\theta_0$, while an order 4 model takes the form $\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c06c3-f3bb-452f-acef-6df2661affbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_model_dataset(degree, dataset):\n",
    "    pipelined_model = Pipeline([\n",
    "            ('polynomial_transformation', PolynomialFeatures(degree)),\n",
    "            ('linear_regression', lm.LinearRegression())    \n",
    "        ])\n",
    "\n",
    "    pipelined_model.fit(dataset[[\"hp\"]], dataset[\"mpg\"])\n",
    "    return mse(dataset['mpg'], pipelined_model.predict(dataset[[\"hp\"]]))\n",
    "\n",
    "errors = [fit_model_dataset(degree, vehicles) for degree in range(0, 8)]\n",
    "MSEs_and_k = pd.DataFrame({\"k\": range(0, 8), \"MSE\": errors})\n",
    "\n",
    "plt.plot(range(0, 8), errors)\n",
    "plt.xlabel(\"Model Complexity (degree of polynomial)\")\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "def plot_degree_k_model(k, MSEs_and_k, axs):\n",
    "    pipelined_model = Pipeline([\n",
    "        ('poly_transform', PolynomialFeatures(degree = k)),\n",
    "        ('regression', lm.LinearRegression(fit_intercept = True))    \n",
    "    ])\n",
    "    pipelined_model.fit(vehicles[[\"hp\"]], vehicles[\"mpg\"])\n",
    "    \n",
    "    row = k // 4\n",
    "    col = k % 4\n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    sns.scatterplot(data=vehicles, x='hp', y='mpg', ax=ax)\n",
    "    \n",
    "    x_range = np.linspace(45, 210, 100).reshape(-1, 1)\n",
    "    ax.plot(x_range, pipelined_model.predict(pd.DataFrame(x_range, columns=['hp'])), c='tab:red', linewidth=2)\n",
    "    \n",
    "    ax.set_ylim((0, 50))\n",
    "    mse_str = f\"MSE: {MSEs_and_k.loc[k, 'MSE']:.4}\\nDegree: {k}\"\n",
    "    ax.text(130, 35, mse_str, dict(size=14))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6), dpi=150)\n",
    "axs = fig.subplots(nrows=2, ncols=4)\n",
    "\n",
    "for k in range(8):\n",
    "    plot_degree_k_model(k, MSEs_and_k, axs)\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197e1e3-681c-4192-8cd1-b7c97c936cbe",
   "metadata": {},
   "source": [
    "As the model increases in polynomial degree (that is, it increases in complexity), the training MSE decreases, plateauing at roughly ~18.\n",
    "\n",
    "In fact, it is a mathematical fact that if we create a polynomial model with degree $n-1$, we can *perfectly* model a set of $n$ points. For example, a set of 5 points can be perfectly modeled by a degree 4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb0a9a-7052-4dcc-83a3-3b72546688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    points = 3*np.random.uniform(size=(5, 2))\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(4)),\n",
    "                ('linear_regression', lm.LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(points[:, [0]], points[:, 1])\n",
    "\n",
    "    ax[i].scatter(points[:, 0], points[:, 1])\n",
    "\n",
    "    xs = np.linspace(0, 3)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91667111-8fef-4fed-9c23-6f204796737a",
   "metadata": {},
   "source": [
    "You may be tempted to always design models with high polynomial degree – after all, we know that we could theoretically achieve perfect predictions by creating a model with enough polynomial features. \n",
    "\n",
    "It turns out that the examples we looked at above represent a somewhat artificial scenario: we trained our model on all the data we had available, then used the model to make predictions on this very same dataset. A more realistic situation is when we wish to apply our model on unseen data – that is, datapoints that it did not encounter during the model fitting process. \n",
    "\n",
    "Suppose we obtain a random sample of 6 datapoints from our population of vehicle data. We want to train a model on these 6 points and use it to make predictions on unseen data (perhaps cars for which we don't already know the true `mpg`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56a90f-6a45-4383-9b91-a1527744f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "sample_6 = vehicles.sample(6)\n",
    "\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\")\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3200ca-4f9a-4bd3-a63b-c2152e38e86d",
   "metadata": {},
   "source": [
    "If we design a model with polynomial degree 5, we can make perfect predictions on this sample of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf0537-6172-44f1-9685-8026576313ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_5_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', lm.LinearRegression())    \n",
    "            ])\n",
    "\n",
    "degree_5_model.fit(sample_6[[\"hp\"]], sample_6[\"mpg\"])\n",
    "xs = np.linspace(0, 250, 1000)\n",
    "degree_5_model_predictions = degree_5_model.predict(xs[:, np.newaxis])\n",
    "\n",
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19c1df-fb2d-4d5f-a623-10cc7c241976",
   "metadata": {},
   "source": [
    "However, when we reapply this fitted model to the full population of data, it fails to capture the major trends of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b9bb6-5c12-47c4-85d9-7c7fa6b61ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59caca6-18a7-4568-b4ec-e5f4ee02a505",
   "metadata": {},
   "source": [
    "The model has **overfit** to the data used to train it. It has essentially \"memorized\" the six datapoints used during model fitting, and does not generalize well to new data. \n",
    "\n",
    "Complex models tend to be more sensitive to the data used to train them. The **variance** of a model refers to its tendency to vary depending on the training data used during model fitting. It turns out that our degree-5 model has very high model variance. If we randomly sample new sets of datapoints to use in training, the model varies erratically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc6d82-afb9-4a4a-bbc2-73d0b8904400",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sample = vehicles.sample(6)\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', lm.LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    ax[i].scatter(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    xs = np.linspace(50, 210, 1000)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\")\n",
    "    ax[i].set_ylim(-80, 100)\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].set_title(f\"Resample #{i+1}\")\n",
    "    \n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c36c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
